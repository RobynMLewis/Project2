---
title: "Project 2- Robyn Lewis"
author: "Robyn Lewis"
date: "10/4/2020"
output: 
  rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(class)
library(tree)
library(caret)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(rmarkdown)
library(knitr)
```

# Introduction

describe data and variables, purpose of analysis

For this project, we'll be analyzing the popularity of articles published on [Mashable](mashable.com). Popularity is determined by number of shares, with 1400 or interactions considered popular. Our data is available through the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity), and consists of 39,644 observations of 61 variables. 

# Data

```{r dataImport}
#read in with relative path
newsData <- read_csv("./Raw Data/OnlineNewsPopularity.csv")

#standardize data

#remember to use TRAIN mean/sd to standardize test data
#use `caret`- can also remove highly correlated vars
```


```{r addPopular}
#popularity is classified as having >1400 shares- creating binary variable to reflect popular/not
popular <- rep(0, nrow(newsData))
popular <- ifelse(newsData$shares >= 1400, "Yes", "No")
newsData <- cbind(newsData, popular)
view(newsData)
```

```{r mondayData}
#suggestion to look at just Monday data first
mondayData <- newsData %>% filter(weekday_is_monday==1)
```

```{r}
#create training data set (70%) and test set (30%)
set.seed(1)
train <- sample(1:nrow(mondayData), size=nrow(mondayData)*0.7)
test <- dplyr::setdiff(1:nrow(mondayData), train)

newsDataTrain <- mondayData[train,]
newsDataTest <- mondayData[test,]

```


# Summarizations

First we'll examine numerical summaries of selected attributes for the articles- number of words in the title, words in the content, number of links, number of images, and numbers of videos. These are separated into "popular" and "unpopular" articles so that we may compare the attributes of both individually. Number of shares is included here as well, but as it is the defining value for popularity, has a specified range for each category. However, we may be able to gain insight to the variability of the number of shares through this table. 

```{r numSums}
#numerical summaries of attributes

#for popular articles
newsDataTrain %>% filter(popular=="Yes") %>% select(c(3, 4, 8, 10, 11, 61)) %>% sapply(summary) %>% kable(digits=2, caption="Numeric Summaries of Popular Article Attributes", col.names=c("Words in Title", "Words in Content", "Links", "Images", "Videos", "Shares"))
#for unpopular articles
newsDataTrain %>% filter(popular=="No") %>% select(c(3, 4, 8, 10, 11, 61)) %>% sapply(summary) %>% kable(digits=2, caption="Numeric Summaries of Unpopular Article Attributes", col.names=c("Words in Title", "Words in Content", "Links", "Images", "Videos", "Shares"))
```


```{r histData, include=FALSE}
counts <- rep(NA, 6)
percents <- rep(NA, 6)
for (i in 14:19){
 counts[i-13] <- sum(newsDataTrain[,i]==1 & newsDataTrain$popular=="Yes") 
 percents[i-13] <- counts[i-13]/sum(newsDataTrain[,i]==1)
}
counts <- as.data.frame(counts)
percents <- as.data.frame(percents)

channel.names <-c("Lifestyle", "Entertainment", "Business", "Social Media", "Tech", "World")
counts <- cbind(counts, channel.names)
colnames(counts) <- c("article.count", "data.channel")
percents <- cbind(percents, channel.names)
colnames(percents) <- c("percent.popular", "data.channel")
```

Next we'll look at articles broken down by category. Here we can visualize the total number of widely shared articles by category, as well as the percentage of articles in each category that are considered popular. 

```{r histograms}
ggplot(data=counts, aes(x=data.channel, y=article.count)) +
  geom_bar(stat="identity", aes(fill=as.factor(data.channel))) +
  labs(title="Count of Popular Articles by Data Channel", x="Data Channel", y="Number of Popular Articles") +
  theme(legend.position = "none")

ggplot(data=percents, aes(x=data.channel, y=percent.popular)) +
  geom_bar(stat="identity", aes(fill=as.factor(data.channel))) +
  labs(title="Percentage of Popular Articles by Data Channel", x="Data Channel", y="Percentage of Popular Articles") +
  theme(legend.position = "none")
```

We will also look at number of shares compared to how the title ranks in both subjectivity and polarity. Articles are assigned a value for each on a scale of 0 to 1. 
```{r}
s1 <- ggplot(data=newsDataTrain, aes(y=shares)) +
  geom_point(aes(x=title_subjectivity), color="blue") +
  labs(title="Shares by Title Subjectivity", x= "Title Subjectivity", y="Shares")

s2 <- ggplot(data=newsDataTrain, aes(y=shares)) +
  geom_point(aes(x=abs_title_sentiment_polarity), color="red") +
  labs(title="Shares by Title Polarity", x= "Title Polarity", y="Shares")

grid.arrange(s1, s2, nrow=1)
  
```
Now we will assess the rates of both positive words and negative words in the articles. 

```{r boxplots}
b1 <- ggplot(data=newsDataTrain, aes(x=popular, y=rate_positive_words)) +
  geom_boxplot(aes(color=popular)) +
  labs(title="Rate of Positive Words in Article", x="Popular Articles", y="Rate of Positive Words") +
  theme(legend.position = "none")

b2 <- ggplot(data=newsDataTrain, aes(x=popular, y=rate_negative_words)) +
  geom_boxplot(aes(color=popular)) +
  labs(title="Rate of Negative Words in Article", x="Popular Articles", y="Rate of Negative Words") +
  theme(legend.position = "none")

grid.arrange(b1, b2, nrow=1)
```
# Modeling

models predicting shares response- describe type of model, fitting process, final chosen model

```{r}
#tree based model, chosen using leave one out cross validation
#Set controls for our training method
trCtrl <- trainControl(method="repeatedcv", number=10, repeats=3)
set.seed(2)
#formulate our training models
treeFit <- train(class~., data=diabetesTrain, method="rpart",
                trControl=trCtrl,
                preProcess=c("center", "scale"))
treeFit
#use cv.tree, prune.misclass
```

```{r}
#boosted tree model chosen using cross validation
boostTree <-
```

# Automation

adapt code to automatically generate a report for each day

```{r dayOfWeek}
#need to consolidate weekday_is_* variables into one column. 
dayOfWeek <- rep(0, nrow(newsData))
dayNames <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
  for(j in 1:length(dayOfWeek)){
    for (i in 32:38){
  if(newsData[,i]==1){dayOfWeek[j] <- dayNames[i-31]}
  }
}
dayOfWeek
```



```{r automation}
day <- unique(newsData$dayOfWeek)
output_file <- paste0(day, "Analysis.md")
params= lapply(day, FUN=function(x){list(day=x)})
reports <- tibble(output_file, params)

apply(reports, MARGIN=1, FUN=function(x){render(input="/Online News Project.Rmd", output_file=x[[1]], params=x[[2]])})
```

# Packages used
```{r}
#class
#tree
#caret
#tidyverse
#dplyr
#ggplot2
#gridExtra
#rmarkdown
#knitr
```

Data Source Used:

K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision
    Support System for Predicting the Popularity of Online News. Proceedings
    of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence,
    September, Coimbra, Portugal.

