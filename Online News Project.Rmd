---
title: "Project 2- Robyn Lewis"
author: "Robyn Lewis"
date: "10/4/2020"
output: 
  rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(class)
library(tree)
library(caret)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(rmarkdown)
library(knitr)
```

# Introduction

describe data and variables, purpose of analysis

For this project, we'll be analyzing the popularity of articles published on [Mashable](mashable.com). Popularity is determined by number of shares, with 1400 or interactions considered popular. Our data is available through the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity), and consists of 39,644 observations of 61 variables. 

# Data

```{r dataImport}
#read in with relative path
newsData <- read_csv("./Raw Data/OnlineNewsPopularity.csv")

#standardize data

#remember to use TRAIN mean/sd to standardize test data
#use `caret`- can also remove highly correlated vars
```


```{r addPopular}
#popularity is classified as having >1400 shares- creating binary variable to reflect popular/not
popular <- rep(0, nrow(newsData))
popular <- ifelse(newsData$shares >= 1400, "Yes", "No")
newsData <- cbind(newsData, popular)
view(newsData)
```

```{r mondayData}
#suggestion to look at just Monday data first
mondayData <- newsData %>% filter(weekday_is_monday==1)
```

```{r}
#create training data set (70%) and test set (30%)
set.seed(1)
train <- sample(1:nrow(mondayData), size=nrow(mondayData)*0.7)
test <- dplyr::setdiff(1:nrow(mondayData), train)

newsDataTrain <- mondayData[train,]
newsDataTest <- mondayData[test,]

#train <- sample(1:nrow(newsData), size=nrow(newsData)*0.7)
#test <- dplyr::setdiff(1:nrow(newsData), train)

#newsDataTrain <- newsData[train,]
#newsDataTest <- newsData[test,]

```


# Summarizations

basic summary statistics and plots about training data 

```{r}
#numerical summaries of some things

newsDataTrain %>% filter(popular=="Yes") %>% select(c(3, 4, 8, 10, 11)) %>% sapply(summary) %>% kable(digits=2, caption="Numeric Summaries of Popular Article Attributes", col.names=c("Words in Title", "Words in Content", "Links", "Images", "Videos"))

newsDataTrain %>% filter(popular=="No") %>% select(c(3, 4, 8, 10, 11)) %>% sapply(summary) %>% kable(digits=2, caption="Numeric Summaries of Unpopular Article Attributes", col.names=c("Words in Title", "Words in Content", "Links", "Images", "Videos"))
```


```{r}
#histogram of data channels of popular articles
#subset to mondayData[, 14:19]
#by popularity
#side by side, 
#14:19

#try sum(is.true())

#maybe look at percentage that are popular?
counts <- rep(NA, 6)
percents <- rep(NA, 6)
for (i in 14:19){
 counts[i-13] <- sum(newsDataTrain[,i]==1 & newsDataTrain$popular=="Yes") 
 percents[i-13] <- counts[i-13]/sum(newsDataTrain[,i]==1)
}
counts <- as.data.frame(counts)
percents <- as.data.frame(percents)
```

```{r}
channel.names <-c("Lifestyle", "Entertainment", "Business", "Social Media", "Tech", "World")
counts <- cbind(counts, channel.names)
colnames(counts) <- c("article.count", "data.channel")
percents <- cbind(percents, channel.names)
colnames(percents) <- c("percent.popular", "data.channel")
```


```{r}
ggplot(data=counts, aes(x=data.channel, y=article.count)) +
  geom_bar(stat="identity", aes(fill=as.factor(data.channel))) +
  labs(title="Count of Popular Articles by Data Channel", x="Data Channel", y="Number of Popular Articles") +
  theme(legend.position = "none")
```

```{r}
ggplot(data=percents, aes(x=data.channel, y=percent.popular)) +
  geom_bar(stat="identity", aes(fill=as.factor(data.channel))) +
  labs(title="Percentage of Popular Articles by Data Channel", x="Data Channel", y="Percentage of Popular Articles") +
  theme(legend.position = "none")
```

# Modeling

models predicting shares response- describe type of model, fitting process, final chosen model

```{r}
#tree based model, chosen using leave one out cross validation

#use cv.tree, prune.misclass
```

```{r}
#boosted tree model chosen using cross validation
```

# Automation

adapt code to automatically generate a report for each day

```{r dayOfWeek}
#need to consolidate weekday_is_* variables into one column. 
dayOfWeek <- rep(0, nrow(newsData))
dayNames <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
for (i in newsData[,32:38]){
  if(newsData[,i]==1){dayOfWeek <- dayNames[i]}
}
dayOfWeek
```



```{r automation}
day <- unique(newsData$dayOfWeek)
output_file <- paste0(day, "Analysis.md")
params= lapply(day, FUN=function(x){list(day=x)})
reports <- tibble(output_file, params)

apply(reports, MARGIN=1, FUN=function(x){render(input="/Online News Project.Rmd", output_file=x[[1]], params=x[[2]])})
```

# Packages used
```{r}
#class
#tree
#caret
#tidyverse
#dplyr
#ggplot2
#rmarkdown
#knitr
```

Data Source Used:

K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision
    Support System for Predicting the Popularity of Online News. Proceedings
    of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence,
    September, Coimbra, Portugal.

